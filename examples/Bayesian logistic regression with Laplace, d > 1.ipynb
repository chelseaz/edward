{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pprint\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import Bernoulli, Normal, MultivariateNormalTriL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MultivariateNormalTriL in module abc:\n",
      "\n",
      "class MultivariateNormalTriL(edward.models.random_variable.RandomVariable, tensorflow.contrib.distributions.python.ops.mvn_tril.MultivariateNormalTriL)\n",
      " |  The multivariate normal distribution on `R^k`.\n",
      " |  \n",
      " |  The Multivariate Normal distribution is defined over `R^k` and parameterized\n",
      " |  by a (batch of) length-`k` `loc` vector (aka \"mu\") and a (batch of) `k x k`\n",
      " |  `scale` matrix; `covariance = scale @ scale.T` where `@` denotes\n",
      " |  matrix-multiplication.\n",
      " |  \n",
      " |  #### Mathematical Details\n",
      " |  \n",
      " |  The probability density function (pdf) is,\n",
      " |  \n",
      " |  ```none\n",
      " |  pdf(x; loc, scale) = exp(-0.5 ||y||**2) / Z,\n",
      " |  y = inv(scale) @ (x - loc),\n",
      " |  Z = (2 pi)**(0.5 k) |det(scale)|,\n",
      " |  ```\n",
      " |  \n",
      " |  where:\n",
      " |  \n",
      " |  * `loc` is a vector in `R^k`,\n",
      " |  * `scale` is a matrix in `R^{k x k}`, `covariance = scale @ scale.T`,\n",
      " |  * `Z` denotes the normalization constant, and,\n",
      " |  * `||y||**2` denotes the squared Euclidean norm of `y`.\n",
      " |  \n",
      " |  A (non-batch) `scale` matrix is:\n",
      " |  \n",
      " |  ```none\n",
      " |  scale = scale_tril\n",
      " |  ```\n",
      " |  \n",
      " |  where `scale_tril` is lower-triangular `k x k` matrix with non-zero diagonal,\n",
      " |  i.e., `tf.diag_part(scale_tril) != 0`.\n",
      " |  \n",
      " |  Additional leading dimensions (if any) will index batches.\n",
      " |  \n",
      " |  The MultivariateNormal distribution is a member of the [location-scale\n",
      " |  family](https://en.wikipedia.org/wiki/Location-scale_family), i.e., it can be\n",
      " |  constructed as,\n",
      " |  \n",
      " |  ```none\n",
      " |  X ~ MultivariateNormal(loc=0, scale=1)   # Identity scale, zero shift.\n",
      " |  Y = scale @ X + loc\n",
      " |  ```\n",
      " |  \n",
      " |  Trainable (batch) lower-triangular matrices can be created with\n",
      " |  `tf.contrib.distributions.matrix_diag_transform()` and/or\n",
      " |  `tf.contrib.distributions.fill_triangular()`\n",
      " |  \n",
      " |  #### Examples\n",
      " |  \n",
      " |  ```python\n",
      " |  tfd = tf.contrib.distributions\n",
      " |  \n",
      " |  # Initialize a single 3-variate Gaussian.\n",
      " |  mu = [1., 2, 3]\n",
      " |  cov = [[ 0.36,  0.12,  0.06],\n",
      " |         [ 0.12,  0.29, -0.13],\n",
      " |         [ 0.06, -0.13,  0.26]]\n",
      " |  scale = tf.cholesky(cov)\n",
      " |  # ==> [[ 0.6,  0. ,  0. ],\n",
      " |  #      [ 0.2,  0.5,  0. ],\n",
      " |  #      [ 0.1, -0.3,  0.4]])\n",
      " |  mvn = tfd.MultivariateNormalTriL(\n",
      " |      loc=mu,\n",
      " |      scale_tril=scale)\n",
      " |  \n",
      " |  mvn.mean().eval()\n",
      " |  # ==> [1., 2, 3]\n",
      " |  \n",
      " |  # Covariance agrees with cholesky(cov) parameterization.\n",
      " |  mvn.covariance().eval()\n",
      " |  # ==> [[ 0.36,  0.12,  0.06],\n",
      " |  #      [ 0.12,  0.29, -0.13],\n",
      " |  #      [ 0.06, -0.13,  0.26]]\n",
      " |  \n",
      " |  # Compute the pdf of an observation in `R^3` ; return a scalar.\n",
      " |  mvn.prob([-1., 0, 1]).eval()  # shape: []\n",
      " |  \n",
      " |  # Initialize a 2-batch of 3-variate Gaussians.\n",
      " |  mu = [[1., 2, 3],\n",
      " |        [11, 22, 33]]              # shape: [2, 3]\n",
      " |  tril = ...  # shape: [2, 3, 3], lower triangular, non-zero diagonal.\n",
      " |  mvn = tfd.MultivariateNormalTriL(\n",
      " |      loc=mu,\n",
      " |      scale_tril=tril)\n",
      " |  \n",
      " |  # Compute the pdf of two `R^3` observations; return a length-2 vector.\n",
      " |  x = [[-0.9, 0, 0.1],\n",
      " |       [-10, 0, 9]]     # shape: [2, 3]\n",
      " |  mvn.prob(x).eval()    # shape: [2]\n",
      " |  \n",
      " |  # Instantiate a \"learnable\" MVN.\n",
      " |  dims = 4\n",
      " |  with tf.variable_scope(\"model\"):\n",
      " |    mvn = tfd.MultivariateNormalTriL(\n",
      " |        loc=tf.get_variable(shape=[dims], dtype=tf.float32, name=\"mu\"),\n",
      " |        scale_tril=tfd.fill_triangular(\n",
      " |            tf.get_variable(shape=[dims * (dims + 1) / 2],\n",
      " |                            dtype=tf.float32, name=\"chol_Sigma\")))\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MultivariateNormalTriL\n",
      " |      edward.models.random_variable.RandomVariable\n",
      " |      tensorflow.contrib.distributions.python.ops.mvn_tril.MultivariateNormalTriL\n",
      " |      tensorflow.contrib.distributions.python.ops.mvn_linear_operator.MultivariateNormalLinearOperator\n",
      " |      tensorflow.python.ops.distributions.transformed_distribution.TransformedDistribution\n",
      " |      tensorflow.python.ops.distributions.distribution.Distribution\n",
      " |      tensorflow.python.ops.distributions.distribution._BaseDistribution\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Construct Multivariate Normal distribution on `R^k`.\n",
      " |      \n",
      " |      The `batch_shape` is the broadcast shape between `loc` and `scale`\n",
      " |      arguments.\n",
      " |      \n",
      " |      The `event_shape` is given by last dimension of the matrix implied by\n",
      " |      `scale`. The last dimension of `loc` (if provided) must broadcast with this.\n",
      " |      \n",
      " |      Recall that `covariance = scale @ scale.T`. A (non-batch) `scale` matrix is:\n",
      " |      \n",
      " |      ```none\n",
      " |      scale = scale_tril\n",
      " |      ```\n",
      " |      \n",
      " |      where `scale_tril` is lower-triangular `k x k` matrix with non-zero\n",
      " |      diagonal, i.e., `tf.diag_part(scale_tril) != 0`.\n",
      " |      \n",
      " |      Additional leading dimensions (if any) will index batches.\n",
      " |      \n",
      " |      Args:\n",
      " |        loc: Floating-point `Tensor`. If this is set to `None`, `loc` is\n",
      " |          implicitly `0`. When specified, may have shape `[B1, ..., Bb, k]` where\n",
      " |          `b >= 0` and `k` is the event size.\n",
      " |        scale_tril: Floating-point, lower-triangular `Tensor` with non-zero\n",
      " |          diagonal elements. `scale_tril` has shape `[B1, ..., Bb, k, k]` where\n",
      " |          `b >= 0` and `k` is the event size.\n",
      " |        validate_args: Python `bool`, default `False`. When `True` distribution\n",
      " |          parameters are checked for validity despite possibly degrading runtime\n",
      " |          performance. When `False` invalid inputs may silently render incorrect\n",
      " |          outputs.\n",
      " |        allow_nan_stats: Python `bool`, default `True`. When `True`,\n",
      " |          statistics (e.g., mean, mode, variance) use the value \"`NaN`\" to\n",
      " |          indicate the result is undefined. When `False`, an exception is raised\n",
      " |          if one or more of the statistic's batch members are undefined.\n",
      " |        name: Python `str` name prefixed to Ops created by this class.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if neither `loc` nor `scale_tril` are specified.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from edward.models.random_variable.RandomVariable:\n",
      " |  \n",
      " |  __abs__ = _run_op(a, *args)\n",
      " |      Computes the absolute value of a tensor.\n",
      " |      \n",
      " |      Given a tensor `x` of complex numbers, this operation returns a tensor of type\n",
      " |      `float32` or `float64` that is the absolute value of each element in `x`. All\n",
      " |      elements in `x` must be complex numbers of the form \\\\(a + bj\\\\). The\n",
      " |      absolute value is computed as \\\\( \\sqrt{a^2 + b^2}\\\\).  For example:\n",
      " |      ```python\n",
      " |      x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])\n",
      " |      tf.abs(x)  # [5.25594902, 6.60492229]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` or `SparseTensor` of type `float32`, `float64`, `int32`,\n",
      " |          `int64`, `complex64` or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` or `SparseTensor` the same size and type as `x` with absolute\n",
      " |          values.\n",
      " |        Note, for `complex64` or `complex128` input, the returned `Tensor` will be\n",
      " |          of type `float32` or `float64`, respectively.\n",
      " |  \n",
      " |  __add__ = _run_op(a, *args)\n",
      " |      Returns x + y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Add` supports broadcasting. `AddN` does not. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __and__ = _run_op(a, *args)\n",
      " |      Returns the truth value of x AND y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `LogicalAnd` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |  \n",
      " |  __div__ = _run_op(a, *args)\n",
      " |      Divide two values using Python 2 semantics. Used for Tensor.__div__.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __floordiv__ = _run_op(a, *args)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.div(x,y)` for integers, but uses `tf.floor(tf.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      Note that for efficiency, `floordiv` uses C semantics for negative numbers\n",
      " |      (unlike Python and Numpy).\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down (except possibly towards zero for negative integers).\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __ge__ = _run_op(a, *args)\n",
      " |      Returns the truth value of (x >= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `GreaterEqual` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __getitem__ = _run_op(a, *args)\n",
      " |      Overload for Tensor.__getitem__.\n",
      " |      \n",
      " |      This operation extracts the specified region from the tensor.\n",
      " |      The notation is similar to NumPy with the restriction that\n",
      " |      currently only support basic indexing. That means that\n",
      " |      using a non-scalar tensor as input is not currently allowed.\n",
      " |      \n",
      " |      Some useful examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # strip leading and trailing 2 elements\n",
      " |      foo = tf.constant([1,2,3,4,5,6])\n",
      " |      print(foo[2:-2].eval())  # => [3,4]\n",
      " |      \n",
      " |      # skip every row and reverse every column\n",
      " |      foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\n",
      " |      print(foo[::2,::-1].eval())  # => [[3,2,1], [9,8,7]]\n",
      " |      \n",
      " |      # Use scalar tensors as indices on both dimensions\n",
      " |      print(foo[tf.constant(0), tf.constant(2)].eval())  # => 3\n",
      " |      \n",
      " |      # Insert another dimension\n",
      " |      foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\n",
      " |      print(foo[tf.newaxis, :, :].eval()) # => [[[1,2,3], [4,5,6], [7,8,9]]]\n",
      " |      print(foo[:, tf.newaxis, :].eval()) # => [[[1,2,3]], [[4,5,6]], [[7,8,9]]]\n",
      " |      print(foo[:, :, tf.newaxis].eval()) # => [[[1],[2],[3]], [[4],[5],[6]],\n",
      " |      [[7],[8],[9]]]\n",
      " |      \n",
      " |      # Ellipses (3 equivalent operations)\n",
      " |      foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\n",
      " |      print(foo[tf.newaxis, :, :].eval())  # => [[[1,2,3], [4,5,6], [7,8,9]]]\n",
      " |      print(foo[tf.newaxis, ...].eval())  # => [[[1,2,3], [4,5,6], [7,8,9]]]\n",
      " |      print(foo[tf.newaxis].eval())  # => [[[1,2,3], [4,5,6], [7,8,9]]]\n",
      " |      ```\n",
      " |      \n",
      " |      Notes:\n",
      " |        - `tf.newaxis` is `None` as in NumPy.\n",
      " |        - An implicit ellipsis is placed at the end of the `slice_spec`\n",
      " |        - NumPy advanced indexing is currently not supported.\n",
      " |      \n",
      " |      Args:\n",
      " |        tensor: An ops.Tensor object.\n",
      " |        slice_spec: The arguments to Tensor.__getitem__.\n",
      " |        var: In the case of variable slice assignment, the Variable\n",
      " |          object to slice (i.e. tensor is the read-only view of this\n",
      " |          variable).\n",
      " |      \n",
      " |      Returns:\n",
      " |        The appropriate slice of \"tensor\", based on \"slice_spec\".\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If a slice range is negative size.\n",
      " |        TypeError: If the slice indices aren't int, slice, or Ellipsis.\n",
      " |  \n",
      " |  __gt__ = _run_op(a, *args)\n",
      " |      Returns the truth value of (x > y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Greater` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __invert__ = _run_op(a, *args)\n",
      " |      Returns the truth value of NOT x element-wise.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __le__ = _run_op(a, *args)\n",
      " |      Returns the truth value of (x <= y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `LessEqual` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __lt__ = _run_op(a, *args)\n",
      " |      Returns the truth value of (x < y) element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Less` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __matmul__ = _run_op(a, *args)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
      " |      where the inner 2 dimensions specify valid matrix multiplication arguments,\n",
      " |      and any further outer dimensions match.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # 2-D tensor `a`\n",
      " |      # [[1, 2, 3],\n",
      " |      #  [4, 5, 6]]\n",
      " |      a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      " |      \n",
      " |      # 2-D tensor `b`\n",
      " |      # [[ 7,  8],\n",
      " |      #  [ 9, 10],\n",
      " |      #  [11, 12]]\n",
      " |      b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[ 58,  64],\n",
      " |      #  [139, 154]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      \n",
      " |      # 3-D tensor `a`\n",
      " |      # [[[ 1,  2,  3],\n",
      " |      #   [ 4,  5,  6]],\n",
      " |      #  [[ 7,  8,  9],\n",
      " |      #   [10, 11, 12]]]\n",
      " |      a = tf.constant(np.arange(1, 13, dtype=np.int32),\n",
      " |                      shape=[2, 2, 3])\n",
      " |      \n",
      " |      # 3-D tensor `b`\n",
      " |      # [[[13, 14],\n",
      " |      #   [15, 16],\n",
      " |      #   [17, 18]],\n",
      " |      #  [[19, 20],\n",
      " |      #   [21, 22],\n",
      " |      #   [23, 24]]]\n",
      " |      b = tf.constant(np.arange(13, 25, dtype=np.int32),\n",
      " |                      shape=[2, 3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[[ 94, 100],\n",
      " |      #   [229, 244]],\n",
      " |      #  [[508, 532],\n",
      " |      #   [697, 730]]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      # Since python >= 3.5 the @ operator is supported (see PEP 465).\n",
      " |      # In TensorFlow, it simply calls the `tf.matmul()` function, so the\n",
      " |      # following lines are equivalent:\n",
      " |      d = a @ b @ [[10.], [11.]]\n",
      " |      d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,\n",
      " |          `complex128` and rank > 1.\n",
      " |        b: `Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of the same type as `a` and `b` where each inner-most matrix is\n",
      " |        the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output`[..., i, j] = sum_k (`a`[..., i, k] * `b`[..., k, j]),\n",
      " |        for all indices i, j.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b\n",
      " |          are both set to True.\n",
      " |  \n",
      " |  __mod__ = _run_op(a, *args)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `FloorMod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __mul__ = _run_op(a, *args)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __neg__ = _run_op(a, *args)\n",
      " |      Computes numerical negative value element-wise.\n",
      " |      \n",
      " |      I.e., \\\\(y = -x\\\\).\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __nonzero__(self)\n",
      " |  \n",
      " |  __or__ = _run_op(a, *args)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `LogicalOr` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __pow__ = _run_op(a, *args)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([[2, 2], [3, 3]])\n",
      " |      y = tf.constant([[8, 16], [2, 3]])\n",
      " |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float32`, `float64`, `int32`, `int64`, `complex64`,\n",
      " |         or `complex128`.\n",
      " |        y: A `Tensor` of type `float32`, `float64`, `int32`, `int64`, `complex64`,\n",
      " |         or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __radd__ = _run_op(a, *args)\n",
      " |      Returns x + y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Add` supports broadcasting. `AddN` does not. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rand__ = _run_op(a, *args)\n",
      " |      Returns the truth value of x AND y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `LogicalAnd` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __rdiv__ = _run_op(a, *args)\n",
      " |      Divide two values using Python 2 semantics. Used for Tensor.__div__.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      Returns:\n",
      " |        `x / y` returns the quotient of x and y.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rfloordiv__ = _run_op(a, *args)\n",
      " |      Divides `x / y` elementwise, rounding toward the most negative integer.\n",
      " |      \n",
      " |      The same as `tf.div(x,y)` for integers, but uses `tf.floor(tf.div(x,y))` for\n",
      " |      floating point arguments so that the result is always an integer (though\n",
      " |      possibly an integer represented as floating point).  This op is generated by\n",
      " |      `x // y` floor division in Python 3 and in Python 2.7 with\n",
      " |      `from __future__ import division`.\n",
      " |      \n",
      " |      Note that for efficiency, `floordiv` uses C semantics for negative numbers\n",
      " |      (unlike Python and Numpy).\n",
      " |      \n",
      " |      `x` and `y` must have the same type, and the result will have the same type\n",
      " |      as well.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: `Tensor` numerator of real numeric type.\n",
      " |        y: `Tensor` denominator of real numeric type.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        `x / y` rounded down (except possibly towards zero for negative integers).\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If the inputs are complex.\n",
      " |  \n",
      " |  __rmatmul__ = _run_op(a, *args)\n",
      " |      Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n",
      " |      \n",
      " |      The inputs must, following any transpositions, be tensors of rank >= 2\n",
      " |      where the inner 2 dimensions specify valid matrix multiplication arguments,\n",
      " |      and any further outer dimensions match.\n",
      " |      \n",
      " |      Both matrices must be of the same type. The supported types are:\n",
      " |      `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.\n",
      " |      \n",
      " |      Either matrix can be transposed or adjointed (conjugated and transposed) on\n",
      " |      the fly by setting one of the corresponding flag to `True`. These are `False`\n",
      " |      by default.\n",
      " |      \n",
      " |      If one or both of the matrices contain a lot of zeros, a more efficient\n",
      " |      multiplication algorithm can be used by setting the corresponding\n",
      " |      `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.\n",
      " |      This optimization is only available for plain matrices (rank-2 tensors) with\n",
      " |      datatypes `bfloat16` or `float32`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      # 2-D tensor `a`\n",
      " |      # [[1, 2, 3],\n",
      " |      #  [4, 5, 6]]\n",
      " |      a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n",
      " |      \n",
      " |      # 2-D tensor `b`\n",
      " |      # [[ 7,  8],\n",
      " |      #  [ 9, 10],\n",
      " |      #  [11, 12]]\n",
      " |      b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[ 58,  64],\n",
      " |      #  [139, 154]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      \n",
      " |      # 3-D tensor `a`\n",
      " |      # [[[ 1,  2,  3],\n",
      " |      #   [ 4,  5,  6]],\n",
      " |      #  [[ 7,  8,  9],\n",
      " |      #   [10, 11, 12]]]\n",
      " |      a = tf.constant(np.arange(1, 13, dtype=np.int32),\n",
      " |                      shape=[2, 2, 3])\n",
      " |      \n",
      " |      # 3-D tensor `b`\n",
      " |      # [[[13, 14],\n",
      " |      #   [15, 16],\n",
      " |      #   [17, 18]],\n",
      " |      #  [[19, 20],\n",
      " |      #   [21, 22],\n",
      " |      #   [23, 24]]]\n",
      " |      b = tf.constant(np.arange(13, 25, dtype=np.int32),\n",
      " |                      shape=[2, 3, 2])\n",
      " |      \n",
      " |      # `a` * `b`\n",
      " |      # [[[ 94, 100],\n",
      " |      #   [229, 244]],\n",
      " |      #  [[508, 532],\n",
      " |      #   [697, 730]]]\n",
      " |      c = tf.matmul(a, b)\n",
      " |      \n",
      " |      # Since python >= 3.5 the @ operator is supported (see PEP 465).\n",
      " |      # In TensorFlow, it simply calls the `tf.matmul()` function, so the\n",
      " |      # following lines are equivalent:\n",
      " |      d = a @ b @ [[10.], [11.]]\n",
      " |      d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,\n",
      " |          `complex128` and rank > 1.\n",
      " |        b: `Tensor` with same type and rank as `a`.\n",
      " |        transpose_a: If `True`, `a` is transposed before multiplication.\n",
      " |        transpose_b: If `True`, `b` is transposed before multiplication.\n",
      " |        adjoint_a: If `True`, `a` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        adjoint_b: If `True`, `b` is conjugated and transposed before\n",
      " |          multiplication.\n",
      " |        a_is_sparse: If `True`, `a` is treated as a sparse matrix.\n",
      " |        b_is_sparse: If `True`, `b` is treated as a sparse matrix.\n",
      " |        name: Name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of the same type as `a` and `b` where each inner-most matrix is\n",
      " |        the product of the corresponding matrices in `a` and `b`, e.g. if all\n",
      " |        transpose or adjoint attributes are `False`:\n",
      " |      \n",
      " |        `output`[..., i, j] = sum_k (`a`[..., i, k] * `b`[..., k, j]),\n",
      " |        for all indices i, j.\n",
      " |      \n",
      " |        Note: This is matrix product, not element-wise product.\n",
      " |      \n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b\n",
      " |          are both set to True.\n",
      " |  \n",
      " |  __rmod__ = _run_op(a, *args)\n",
      " |      Returns element-wise remainder of division. When `x < 0` xor `y < 0` is\n",
      " |      \n",
      " |      true, this follows Python semantics in that the result here is consistent\n",
      " |      with a flooring divide. E.g. `floor(x / y) * y + mod(x, y) = x`.\n",
      " |      \n",
      " |      *NOTE*: `FloorMod` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `int32`, `int64`, `bfloat16`, `float32`, `float64`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rmul__ = _run_op(a, *args)\n",
      " |      Dispatches cwise mul for \"Dense*Dense\" and \"Dense*Sparse\".\n",
      " |  \n",
      " |  __ror__ = _run_op(a, *args)\n",
      " |      Returns the truth value of x OR y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `LogicalOr` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `bool`.\n",
      " |        y: A `Tensor` of type `bool`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor` of type `bool`.\n",
      " |  \n",
      " |  __rpow__ = _run_op(a, *args)\n",
      " |      Computes the power of one value to another.\n",
      " |      \n",
      " |      Given a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\n",
      " |      corresponding elements in `x` and `y`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      x = tf.constant([[2, 2], [3, 3]])\n",
      " |      y = tf.constant([[8, 16], [2, 3]])\n",
      " |      tf.pow(x, y)  # [[256, 65536], [9, 27]]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor` of type `float32`, `float64`, `int32`, `int64`, `complex64`,\n",
      " |         or `complex128`.\n",
      " |        y: A `Tensor` of type `float32`, `float64`, `int32`, `int64`, `complex64`,\n",
      " |         or `complex128`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`.\n",
      " |  \n",
      " |  __rsub__ = _run_op(a, *args)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __rtruediv__ = _run_op(a, *args)\n",
      " |  \n",
      " |  __rxor__ = _run_op(a, *args)\n",
      " |      x ^ y = (x | y) & ~(x & y).\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __sub__ = _run_op(a, *args)\n",
      " |      Returns x - y element-wise.\n",
      " |      \n",
      " |      *NOTE*: `Subtract` supports broadcasting. More about broadcasting\n",
      " |      [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n",
      " |      \n",
      " |      Args:\n",
      " |        x: A `Tensor`. Must be one of the following types: `half`, `bfloat16`, `float32`, `float64`, `uint8`, `int8`, `uint16`, `int16`, `int32`, `int64`, `complex64`, `complex128`.\n",
      " |        y: A `Tensor`. Must have the same type as `x`.\n",
      " |        name: A name for the operation (optional).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `Tensor`. Has the same type as `x`.\n",
      " |  \n",
      " |  __truediv__ = _run_op(a, *args)\n",
      " |  \n",
      " |  __xor__ = _run_op(a, *args)\n",
      " |      x ^ y = (x | y) & ~(x & y).\n",
      " |  \n",
      " |  eval(self, session=None, feed_dict=None)\n",
      " |      In a session, computes and returns the value of this random variable.\n",
      " |      \n",
      " |      This is not a graph construction method, it does not add ops to the graph.\n",
      " |      \n",
      " |      This convenience method requires a session where the graph\n",
      " |      containing this variable has been launched. If no session is\n",
      " |      passed, the default session is used.\n",
      " |      \n",
      " |      Args:\n",
      " |        session: tf.BaseSession, optional.\n",
      " |          The `tf.Session` to use to evaluate this random variable. If\n",
      " |          none, the default session is used.\n",
      " |        feed_dict: dict, optional.\n",
      " |          A dictionary that maps `tf.Tensor` objects to feed values. See\n",
      " |          `tf.Session.run()` for a description of the valid feed values.\n",
      " |      \n",
      " |      #### Examples\n",
      " |      \n",
      " |      ```python\n",
      " |      x = Normal(0.0, 1.0)\n",
      " |      with tf.Session() as sess:\n",
      " |        # Usage passing the session explicitly.\n",
      " |        print(x.eval(sess))\n",
      " |        # Usage with the default session.  The 'with' block\n",
      " |        # above makes 'sess' the default session.\n",
      " |        print(x.eval())\n",
      " |      ```\n",
      " |  \n",
      " |  get_ancestors(self, collection=None)\n",
      " |      Get ancestor random variables.\n",
      " |  \n",
      " |  get_blanket(self, collection=None)\n",
      " |      Get the random variable's Markov blanket.\n",
      " |  \n",
      " |  get_children(self, collection=None)\n",
      " |      Get child random variables.\n",
      " |  \n",
      " |  get_descendants(self, collection=None)\n",
      " |      Get descendant random variables.\n",
      " |  \n",
      " |  get_parents(self, collection=None)\n",
      " |      Get parent random variables.\n",
      " |  \n",
      " |  get_shape(self)\n",
      " |      Get shape of random variable.\n",
      " |  \n",
      " |  get_siblings(self, collection=None)\n",
      " |      Get sibling random variables.\n",
      " |  \n",
      " |  get_variables(self, collection=None)\n",
      " |      Get TensorFlow variables that the random variable depends on.\n",
      " |  \n",
      " |  value(self)\n",
      " |      Get tensor that the random variable corresponds to.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from edward.models.random_variable.RandomVariable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  sample_shape\n",
      " |      Sample shape of random variable.\n",
      " |  \n",
      " |  shape\n",
      " |      Shape of random variable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from edward.models.random_variable.RandomVariable:\n",
      " |  \n",
      " |  __array_priority__ = 100\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.contrib.distributions.python.ops.mvn_linear_operator.MultivariateNormalLinearOperator:\n",
      " |  \n",
      " |  log_prob(self, value, name='log_prob')\n",
      " |      Log probability density/mass function.\n",
      " |      \n",
      " |      \n",
      " |      Additional documentation from `MultivariateNormalLinearOperator`:\n",
      " |      \n",
      " |      `value` is a batch vector with compatible shape if `value` is a `Tensor` whose\n",
      " |      shape can be broadcast up to either:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.batch_shape + self.event_shape\n",
      " |      ```\n",
      " |      \n",
      " |      or\n",
      " |      \n",
      " |      ```python\n",
      " |      [M1, ..., Mm] + self.batch_shape + self.event_shape\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: `float` or `double` `Tensor`.\n",
      " |        name: Python `str` prepended to names of ops created by this function.\n",
      " |      \n",
      " |      Returns:\n",
      " |        log_prob: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with\n",
      " |          values of type `self.dtype`.\n",
      " |  \n",
      " |  prob(self, value, name='prob')\n",
      " |      Probability density/mass function.\n",
      " |      \n",
      " |      \n",
      " |      Additional documentation from `MultivariateNormalLinearOperator`:\n",
      " |      \n",
      " |      `value` is a batch vector with compatible shape if `value` is a `Tensor` whose\n",
      " |      shape can be broadcast up to either:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.batch_shape + self.event_shape\n",
      " |      ```\n",
      " |      \n",
      " |      or\n",
      " |      \n",
      " |      ```python\n",
      " |      [M1, ..., Mm] + self.batch_shape + self.event_shape\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: `float` or `double` `Tensor`.\n",
      " |        name: Python `str` prepended to names of ops created by this function.\n",
      " |      \n",
      " |      Returns:\n",
      " |        prob: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with\n",
      " |          values of type `self.dtype`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.contrib.distributions.python.ops.mvn_linear_operator.MultivariateNormalLinearOperator:\n",
      " |  \n",
      " |  loc\n",
      " |      The `loc` `Tensor` in `Y = scale @ X + loc`.\n",
      " |  \n",
      " |  scale\n",
      " |      The `scale` `LinearOperator` in `Y = scale @ X + loc`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.ops.distributions.transformed_distribution.TransformedDistribution:\n",
      " |  \n",
      " |  bijector\n",
      " |      Function transforming x => y.\n",
      " |  \n",
      " |  distribution\n",
      " |      Base distribution, p(x).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.ops.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  batch_shape_tensor(self, name='batch_shape_tensor')\n",
      " |      Shape of a single sample from a single event index as a 1-D `Tensor`.\n",
      " |      \n",
      " |      The batch dimensions are indexes into independent, non-identical\n",
      " |      parameterizations of this distribution.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: name to give to the op\n",
      " |      \n",
      " |      Returns:\n",
      " |        batch_shape: `Tensor`.\n",
      " |  \n",
      " |  cdf(self, value, name='cdf')\n",
      " |      Cumulative distribution function.\n",
      " |      \n",
      " |      Given random variable `X`, the cumulative distribution function `cdf` is:\n",
      " |      \n",
      " |      ```none\n",
      " |      cdf(x) := P[X <= x]\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: `float` or `double` `Tensor`.\n",
      " |        name: Python `str` prepended to names of ops created by this function.\n",
      " |      \n",
      " |      Returns:\n",
      " |        cdf: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with\n",
      " |          values of type `self.dtype`.\n",
      " |  \n",
      " |  copy(self, **override_parameters_kwargs)\n",
      " |      Creates a deep copy of the distribution.\n",
      " |      \n",
      " |      Note: the copy distribution may continue to depend on the original\n",
      " |      initialization arguments.\n",
      " |      \n",
      " |      Args:\n",
      " |        **override_parameters_kwargs: String/value dictionary of initialization\n",
      " |          arguments to override with new values.\n",
      " |      \n",
      " |      Returns:\n",
      " |        distribution: A new instance of `type(self)` initialized from the union\n",
      " |          of self.parameters and override_parameters_kwargs, i.e.,\n",
      " |          `dict(self.parameters, **override_parameters_kwargs)`.\n",
      " |  \n",
      " |  covariance(self, name='covariance')\n",
      " |      Covariance.\n",
      " |      \n",
      " |      Covariance is (possibly) defined only for non-scalar-event distributions.\n",
      " |      \n",
      " |      For example, for a length-`k`, vector-valued distribution, it is calculated\n",
      " |      as,\n",
      " |      \n",
      " |      ```none\n",
      " |      Cov[i, j] = Covariance(X_i, X_j) = E[(X_i - E[X_i]) (X_j - E[X_j])]\n",
      " |      ```\n",
      " |      \n",
      " |      where `Cov` is a (batch of) `k x k` matrix, `0 <= (i, j) < k`, and `E`\n",
      " |      denotes expectation.\n",
      " |      \n",
      " |      Alternatively, for non-vector, multivariate distributions (e.g.,\n",
      " |      matrix-valued, Wishart), `Covariance` shall return a (batch of) matrices\n",
      " |      under some vectorization of the events, i.e.,\n",
      " |      \n",
      " |      ```none\n",
      " |      Cov[i, j] = Covariance(Vec(X)_i, Vec(X)_j) = [as above]\n",
      " |      ```\n",
      " |      \n",
      " |      where `Cov` is a (batch of) `k' x k'` matrices,\n",
      " |      `0 <= (i, j) < k' = reduce_prod(event_shape)`, and `Vec` is some function\n",
      " |      mapping indices of this distribution's event dimensions to indices of a\n",
      " |      length-`k'` vector.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: Python `str` prepended to names of ops created by this function.\n",
      " |      \n",
      " |      Returns:\n",
      " |        covariance: Floating-point `Tensor` with shape `[B1, ..., Bn, k', k']`\n",
      " |          where the first `n` dimensions are batch coordinates and\n",
      " |          `k' = reduce_prod(self.event_shape)`.\n",
      " |  \n",
      " |  cross_entropy(self, other, name='cross_entropy')\n",
      " |      Computes the (Shannon) cross entropy.\n",
      " |      \n",
      " |      Denote this distribution (`self`) by `P` and the `other` distribution by\n",
      " |      `Q`. Assuming `P, Q` are absolutely continuous with respect to\n",
      " |      one another and permit densities `p(x) dr(x)` and `q(x) dr(x)`, (Shanon)\n",
      " |      cross entropy is defined as:\n",
      " |      \n",
      " |      ```none\n",
      " |      H[P, Q] = E_p[-log q(X)] = -int_F p(x) log q(x) dr(x)\n",
      " |      ```\n",
      " |      \n",
      " |      where `F` denotes the support of the random variable `X ~ P`.\n",
      " |      \n",
      " |      Args:\n",
      " |        other: `tf.distributions.Distribution` instance.\n",
      " |        name: Python `str` prepended to names of ops created by this function.\n",
      " |      \n",
      " |      Returns:\n",
      " |        cross_entropy: `self.dtype` `Tensor` with shape `[B1, ..., Bn]`\n",
      " |          representing `n` different calculations of (Shanon) cross entropy.\n",
      " |  \n",
      " |  entropy(self, name='entropy')\n",
      " |      Shannon entropy in nats.\n",
      " |  \n",
      " |  event_shape_tensor(self, name='event_shape_tensor')\n",
      " |      Shape of a single sample from a single batch as a 1-D int32 `Tensor`.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: name to give to the op\n",
      " |      \n",
      " |      Returns:\n",
      " |        event_shape: `Tensor`.\n",
      " |  \n",
      " |  is_scalar_batch(self, name='is_scalar_batch')\n",
      " |      Indicates that `batch_shape == []`.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: Python `str` prepended to names of ops created by this function.\n",
      " |      \n",
      " |      Returns:\n",
      " |        is_scalar_batch: `bool` scalar `Tensor`.\n",
      " |  \n",
      " |  is_scalar_event(self, name='is_scalar_event')\n",
      " |      Indicates that `event_shape == []`.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: Python `str` prepended to names of ops created by this function.\n",
      " |      \n",
      " |      Returns:\n",
      " |        is_scalar_event: `bool` scalar `Tensor`.\n",
      " |  \n",
      " |  kl_divergence(self, other, name='kl_divergence')\n",
      " |      Computes the Kullback--Leibler divergence.\n",
      " |      \n",
      " |      Denote this distribution (`self`) by `p` and the `other` distribution by\n",
      " |      `q`. Assuming `p, q` are absolutely continuous with respect to reference\n",
      " |      measure `r`, the KL divergence is defined as:\n",
      " |      \n",
      " |      ```none\n",
      " |      KL[p, q] = E_p[log(p(X)/q(X))]\n",
      " |               = -int_F p(x) log q(x) dr(x) + int_F p(x) log p(x) dr(x)\n",
      " |               = H[p, q] - H[p]\n",
      " |      ```\n",
      " |      \n",
      " |      where `F` denotes the support of the random variable `X ~ p`, `H[., .]`\n",
      " |      denotes (Shanon) cross entropy, and `H[.]` denotes (Shanon) entropy.\n",
      " |      \n",
      " |      Args:\n",
      " |        other: `tf.distributions.Distribution` instance.\n",
      " |        name: Python `str` prepended to names of ops created by this function.\n",
      " |      \n",
      " |      Returns:\n",
      " |        kl_divergence: `self.dtype` `Tensor` with shape `[B1, ..., Bn]`\n",
      " |          representing `n` different calculations of the Kullback-Leibler\n",
      " |          divergence.\n",
      " |  \n",
      " |  log_cdf(self, value, name='log_cdf')\n",
      " |      Log cumulative distribution function.\n",
      " |      \n",
      " |      Given random variable `X`, the cumulative distribution function `cdf` is:\n",
      " |      \n",
      " |      ```none\n",
      " |      log_cdf(x) := Log[ P[X <= x] ]\n",
      " |      ```\n",
      " |      \n",
      " |      Often, a numerical approximation can be used for `log_cdf(x)` that yields\n",
      " |      a more accurate answer than simply taking the logarithm of the `cdf` when\n",
      " |      `x << -1`.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: `float` or `double` `Tensor`.\n",
      " |        name: Python `str` prepended to names of ops created by this function.\n",
      " |      \n",
      " |      Returns:\n",
      " |        logcdf: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with\n",
      " |          values of type `self.dtype`.\n",
      " |  \n",
      " |  log_survival_function(self, value, name='log_survival_function')\n",
      " |      Log survival function.\n",
      " |      \n",
      " |      Given random variable `X`, the survival function is defined:\n",
      " |      \n",
      " |      ```none\n",
      " |      log_survival_function(x) = Log[ P[X > x] ]\n",
      " |                               = Log[ 1 - P[X <= x] ]\n",
      " |                               = Log[ 1 - cdf(x) ]\n",
      " |      ```\n",
      " |      \n",
      " |      Typically, different numerical approximations can be used for the log\n",
      " |      survival function, which are more accurate than `1 - cdf(x)` when `x >> 1`.\n",
      " |      \n",
      " |      Args:\n",
      " |        value: `float` or `double` `Tensor`.\n",
      " |        name: Python `str` prepended to names of ops created by this function.\n",
      " |      \n",
      " |      Returns:\n",
      " |        `Tensor` of shape `sample_shape(x) + self.batch_shape` with values of type\n",
      " |          `self.dtype`.\n",
      " |  \n",
      " |  mean(self, name='mean')\n",
      " |      Mean.\n",
      " |  \n",
      " |  mode(self, name='mode')\n",
      " |      Mode.\n",
      " |  \n",
      " |  quantile(self, value, name='quantile')\n",
      " |      Quantile function. Aka \"inverse cdf\" or \"percent point function\".\n",
      " |      \n",
      " |      Given random variable `X` and `p in [0, 1]`, the `quantile` is:\n",
      " |      \n",
      " |      ```none\n",
      " |      quantile(p) := x such that P[X <= x] == p\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: `float` or `double` `Tensor`.\n",
      " |        name: Python `str` prepended to names of ops created by this function.\n",
      " |      \n",
      " |      Returns:\n",
      " |        quantile: a `Tensor` of shape `sample_shape(x) + self.batch_shape` with\n",
      " |          values of type `self.dtype`.\n",
      " |  \n",
      " |  sample(self, sample_shape=(), seed=None, name='sample')\n",
      " |      Generate samples of the specified shape.\n",
      " |      \n",
      " |      Note that a call to `sample()` without arguments will generate a single\n",
      " |      sample.\n",
      " |      \n",
      " |      Args:\n",
      " |        sample_shape: 0D or 1D `int32` `Tensor`. Shape of the generated samples.\n",
      " |        seed: Python integer seed for RNG\n",
      " |        name: name to give to the op.\n",
      " |      \n",
      " |      Returns:\n",
      " |        samples: a `Tensor` with prepended dimensions `sample_shape`.\n",
      " |  \n",
      " |  stddev(self, name='stddev')\n",
      " |      Standard deviation.\n",
      " |      \n",
      " |      Standard deviation is defined as,\n",
      " |      \n",
      " |      ```none\n",
      " |      stddev = E[(X - E[X])**2]**0.5\n",
      " |      ```\n",
      " |      \n",
      " |      where `X` is the random variable associated with this distribution, `E`\n",
      " |      denotes expectation, and `stddev.shape = batch_shape + event_shape`.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: Python `str` prepended to names of ops created by this function.\n",
      " |      \n",
      " |      Returns:\n",
      " |        stddev: Floating-point `Tensor` with shape identical to\n",
      " |          `batch_shape + event_shape`, i.e., the same shape as `self.mean()`.\n",
      " |  \n",
      " |  survival_function(self, value, name='survival_function')\n",
      " |      Survival function.\n",
      " |      \n",
      " |      Given random variable `X`, the survival function is defined:\n",
      " |      \n",
      " |      ```none\n",
      " |      survival_function(x) = P[X > x]\n",
      " |                           = 1 - P[X <= x]\n",
      " |                           = 1 - cdf(x).\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: `float` or `double` `Tensor`.\n",
      " |        name: Python `str` prepended to names of ops created by this function.\n",
      " |      \n",
      " |      Returns:\n",
      " |        `Tensor` of shape `sample_shape(x) + self.batch_shape` with values of type\n",
      " |          `self.dtype`.\n",
      " |  \n",
      " |  variance(self, name='variance')\n",
      " |      Variance.\n",
      " |      \n",
      " |      Variance is defined as,\n",
      " |      \n",
      " |      ```none\n",
      " |      Var = E[(X - E[X])**2]\n",
      " |      ```\n",
      " |      \n",
      " |      where `X` is the random variable associated with this distribution, `E`\n",
      " |      denotes expectation, and `Var.shape = batch_shape + event_shape`.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: Python `str` prepended to names of ops created by this function.\n",
      " |      \n",
      " |      Returns:\n",
      " |        variance: Floating-point `Tensor` with shape identical to\n",
      " |          `batch_shape + event_shape`, i.e., the same shape as `self.mean()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.ops.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  param_shapes(sample_shape, name='DistributionParamShapes') from tensorflow.python.ops.distributions.distribution._DistributionMeta\n",
      " |      Shapes of parameters given the desired shape of a call to `sample()`.\n",
      " |      \n",
      " |      This is a class method that describes what key/value arguments are required\n",
      " |      to instantiate the given `Distribution` so that a particular shape is\n",
      " |      returned for that instance's call to `sample()`.\n",
      " |      \n",
      " |      Subclasses should override class method `_param_shapes`.\n",
      " |      \n",
      " |      Args:\n",
      " |        sample_shape: `Tensor` or python list/tuple. Desired shape of a call to\n",
      " |          `sample()`.\n",
      " |        name: name to prepend ops with.\n",
      " |      \n",
      " |      Returns:\n",
      " |        `dict` of parameter name to `Tensor` shapes.\n",
      " |  \n",
      " |  param_static_shapes(sample_shape) from tensorflow.python.ops.distributions.distribution._DistributionMeta\n",
      " |      param_shapes with static (i.e. `TensorShape`) shapes.\n",
      " |      \n",
      " |      This is a class method that describes what key/value arguments are required\n",
      " |      to instantiate the given `Distribution` so that a particular shape is\n",
      " |      returned for that instance's call to `sample()`. Assumes that the sample's\n",
      " |      shape is known statically.\n",
      " |      \n",
      " |      Subclasses should override class method `_param_shapes` to return\n",
      " |      constant-valued tensors when constant values are fed.\n",
      " |      \n",
      " |      Args:\n",
      " |        sample_shape: `TensorShape` or python list/tuple. Desired shape of a call\n",
      " |          to `sample()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        `dict` of parameter name to `TensorShape`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if `sample_shape` is a `TensorShape` and is not fully defined.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.ops.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  allow_nan_stats\n",
      " |      Python `bool` describing behavior when a stat is undefined.\n",
      " |      \n",
      " |      Stats return +/- infinity when it makes sense. E.g., the variance of a\n",
      " |      Cauchy distribution is infinity. However, sometimes the statistic is\n",
      " |      undefined, e.g., if a distribution's pdf does not achieve a maximum within\n",
      " |      the support of the distribution, the mode is undefined. If the mean is\n",
      " |      undefined, then by definition the variance is undefined. E.g. the mean for\n",
      " |      Student's T for df = 1 is undefined (no clear way to say it is either + or -\n",
      " |      infinity), so the variance = E[(X - mean)**2] is also undefined.\n",
      " |      \n",
      " |      Returns:\n",
      " |        allow_nan_stats: Python `bool`.\n",
      " |  \n",
      " |  batch_shape\n",
      " |      Shape of a single sample from a single event index as a `TensorShape`.\n",
      " |      \n",
      " |      May be partially defined or unknown.\n",
      " |      \n",
      " |      The batch dimensions are indexes into independent, non-identical\n",
      " |      parameterizations of this distribution.\n",
      " |      \n",
      " |      Returns:\n",
      " |        batch_shape: `TensorShape`, possibly unknown.\n",
      " |  \n",
      " |  dtype\n",
      " |      The `DType` of `Tensor`s handled by this `Distribution`.\n",
      " |  \n",
      " |  event_shape\n",
      " |      Shape of a single sample from a single batch as a `TensorShape`.\n",
      " |      \n",
      " |      May be partially defined or unknown.\n",
      " |      \n",
      " |      Returns:\n",
      " |        event_shape: `TensorShape`, possibly unknown.\n",
      " |  \n",
      " |  name\n",
      " |      Name prepended to all ops created by this `Distribution`.\n",
      " |  \n",
      " |  parameters\n",
      " |      Dictionary of parameters used to instantiate this `Distribution`.\n",
      " |  \n",
      " |  reparameterization_type\n",
      " |      Describes how samples from the distribution are reparameterized.\n",
      " |      \n",
      " |      Currently this is one of the static instances\n",
      " |      `distributions.FULLY_REPARAMETERIZED`\n",
      " |      or `distributions.NOT_REPARAMETERIZED`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An instance of `ReparameterizationType`.\n",
      " |  \n",
      " |  validate_args\n",
      " |      Python `bool` indicating possibly expensive checks are enabled.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MultivariateNormalTriL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLAGS:\n",
    "    N=1000   # Number of data points\n",
    "    D=5     # Number of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_toy_dataset(N, D, noise_std=1):\n",
    "    X = np.random.uniform(-6, 6, size=(N, D))\n",
    "    w = np.random.uniform(-1, 1, size=D)\n",
    "    b = np.random.uniform(-4, 4)\n",
    "    epsilon = np.random.normal(0, noise_std, size=N)\n",
    "    y = (np.dot(X, w) + b + epsilon > 0).astype(int)\n",
    "    # note this is actually generated from a probit model\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.73977116 -1.33461061 -0.38567263  0.43450265  0.24020584]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyz/edward/edward/lib/python3.6/site-packages/edward/util/random_variables.py:52: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  not np.issubdtype(value.dtype, np.float) and \\\n",
      "/Users/cyz/edward/edward/lib/python3.6/site-packages/edward/util/random_variables.py:53: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  not np.issubdtype(value.dtype, np.int) and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 1s | Loss: 152.222\n"
     ]
    }
   ],
   "source": [
    "ed.set_seed(42)\n",
    "\n",
    "# DATA\n",
    "X_train, y_train = build_toy_dataset(FLAGS.N, FLAGS.D)\n",
    "\n",
    "# MODEL\n",
    "X = tf.placeholder(tf.float32, [FLAGS.N, FLAGS.D])\n",
    "w = Normal(loc=tf.zeros(FLAGS.D), scale=tf.ones(FLAGS.D))\n",
    "b = Normal(loc=tf.zeros([1]), scale=tf.ones([1]))\n",
    "y = Bernoulli(logits=ed.dot(X, w) + b)\n",
    "\n",
    "# INFERENCE\n",
    "qb = Normal(\n",
    "    loc=tf.Variable(tf.zeros([1])), \n",
    "    scale=tf.Variable(tf.ones([1])))  # should probably initialize to random values\n",
    "\n",
    "w_init = np.random.randn(FLAGS.D)\n",
    "print(w_init)\n",
    "\n",
    "qw = MultivariateNormalTriL(\n",
    "    loc=tf.Variable(tf.cast(w_init, tf.float32)),\n",
    "    scale_tril=tf.Variable(tf.random_normal([FLAGS.D, FLAGS.D])))\n",
    "\n",
    "# inference = ed.KLqp({w: qw, b: qb}, data={X: X_train, y: y_train})\n",
    "inference = ed.Laplace({w: qw, b: qb}, data={X: X_train, y: y_train})\n",
    "inference.initialize(n_print=10, n_iter=600)\n",
    "\n",
    "inference.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAGS.D == 1:\n",
    "    n_posterior_samples = 10\n",
    "\n",
    "    w_post = qw.sample(n_posterior_samples).eval()\n",
    "    b_post = qb.sample(n_posterior_samples).eval()\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "    plt.scatter(X_train, y_train)\n",
    "\n",
    "    inputs = np.linspace(-6, 6, num=400)\n",
    "    for ns in range(n_posterior_samples):\n",
    "        output = scipy.special.expit(np.dot(inputs[:,np.newaxis], w_post[ns]) + b_post[ns])\n",
    "        plt.plot(inputs, output)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04017381,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.00236594,  0.04017104,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.00231206, -0.00321833,  0.04667843,  0.        ,  0.        ],\n",
       "       [ 0.00386958,  0.00074221, -0.00476353,  0.04382423,  0.        ],\n",
       "       [-0.00027969, -0.00264103,  0.00227064, -0.00508947,  0.04496394]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these give same result\n",
    "qw.scale.to_dense().eval()\n",
    "tf.cholesky(qw.covariance()).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.6139353e-03,  9.5049014e-05, -9.2884125e-05,  1.5545593e-04,\n",
       "        -1.1236388e-05],\n",
       "       [ 9.5049014e-05,  1.6193104e-03, -1.3475369e-04,  3.8970698e-05,\n",
       "        -1.0675466e-04],\n",
       "       [-9.2884125e-05, -1.3475369e-04,  2.1945788e-03, -2.3368954e-04,\n",
       "         1.1513616e-04],\n",
       "       [ 1.5545593e-04,  3.8970698e-05, -2.3368954e-04,  1.9587788e-03,\n",
       "        -2.3690081e-04],\n",
       "       [-1.1236388e-05, -1.0675466e-04,  1.1513615e-04, -2.3690083e-04,\n",
       "         2.0598674e-03]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is inverse of observed Fisher information, used in Laplace approximation\n",
    "qw.covariance().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.29878226, -0.0199684 ,  0.92905945, -0.46812683,  1.0524466 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qw.mean().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02032956], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qb.scale.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.5841575], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qb.loc.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify Segall formula is the same\n",
    "w_map = qw.mean().eval()\n",
    "b_map = qb.loc.eval()\n",
    "p = y.mean().eval(feed_dict={X: X_train, w: w_map, b: b_map})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.diag(p * (1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this is the inverse covariance for weighted least squares! The weights are just Bernoulli variances\n",
    "hess_segall = np.matmul(np.matmul(X_train.T, W), X_train) / FLAGS.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.61860367 -0.03649266  0.0281739  -0.05102919 -0.00548801]\n",
      " [-0.03649266  0.62083827  0.0410662  -0.00284558  0.0339018 ]\n",
      " [ 0.0281739   0.0410662   0.46217915  0.04716267 -0.01546445]\n",
      " [-0.05102919 -0.00284558  0.04716267  0.52936101  0.05716532]\n",
      " [-0.00548801  0.0339018  -0.01546445  0.05716532  0.50282823]]\n"
     ]
    }
   ],
   "source": [
    "print(hess_segall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_fisher = np.linalg.inv(qw.covariance().eval()) / FLAGS.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6272852  -0.03436808  0.01965453 -0.04734807 -0.00490336]\n",
      " [-0.03436808  0.6245095   0.03511814 -0.00187936  0.02999928]\n",
      " [ 0.01965453  0.03511814  0.46504438  0.05102199 -0.01819845]\n",
      " [-0.04734807 -0.00187936  0.05102199  0.5273516   0.057442  ]\n",
      " [-0.00490336  0.02999928 -0.01819845  0.057442    0.49461964]]\n"
     ]
    }
   ],
   "source": [
    "print(obs_fisher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The expression Segall derived for the Fisher information is close enough \n",
    "# to the observed Fisher information computed using TF autodiff, available from Laplace approximation.\n",
    "# What causes the discrepancy?\n",
    "# Is the Edward Laplace approx really computed at the mode? We evaluate the Segall expression at the MAP estimate...\n",
    "\n",
    "# Discrepancy increases if the number of samples is one order of magnitude more/less (not sure why)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edward",
   "language": "python",
   "name": "edward"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
