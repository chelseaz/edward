{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pprint\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from edward.models import Bernoulli, Normal, MultivariateNormalTriL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLAGS:\n",
    "    N=1000   # Number of data points\n",
    "    D=5     # Number of features\n",
    "    N_CUT=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_toy_dataset(N, D, noise_std=1, max_level=FLAGS.N_CUT):\n",
    "    X = np.random.uniform(-6, 6, size=(N, D))\n",
    "    w = np.random.uniform(-1, 1, size=D)\n",
    "    epsilon = np.random.normal(0, noise_std, size=N)\n",
    "    \n",
    "    # levels will be 0, 1, ..., max_level\n",
    "    levels = np.array(range(max_level))\n",
    "\n",
    "    # note, due to gaussian noise, this is not generated from ordered logit model\n",
    "    thresholds = levels[:,np.newaxis]\n",
    "    logits = np.dot(X, w) + epsilon\n",
    "    y = (logits > thresholds).astype(int).sum(axis=0)\n",
    "    # level is the last TRUE in each row, which is also the row sum\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edward.models import RandomVariable\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.contrib.distributions import Distribution, NOT_REPARAMETERIZED\n",
    "\n",
    "class distributions_OrdinalLogit(Distribution):\n",
    "    def __init__(self, logits, cutpoints, name=\"OrdinalLogit\"):\n",
    "        self._logits = logits\n",
    "        self._cutpoints = cutpoints\n",
    "        \n",
    "        parameters = dict(locals())\n",
    "        super(distributions_OrdinalLogit, self).__init__(\n",
    "            dtype=dtypes.int32,\n",
    "            reparameterization_type=NOT_REPARAMETERIZED,\n",
    "            validate_args=False,\n",
    "            allow_nan_stats=True,\n",
    "            parameters=parameters,\n",
    "            graph_parents=[self._logits, self._cutpoints],\n",
    "            name=name\n",
    "        )\n",
    "\n",
    "    def _data_size(self):\n",
    "        return tf.size(self._logits)\n",
    "        \n",
    "    def _cumul_probs(self):\n",
    "        logits_with_cutpoints = tf.expand_dims(self._logits, -1) + self._cutpoints  # using broadcasting\n",
    "        return tf.sigmoid(logits_with_cutpoints)\n",
    "        \n",
    "    def _log_prob(self, value):        \n",
    "        cumul_probs = self._cumul_probs()\n",
    "        level_probs = tf.concat([cumul_probs, tf.expand_dims(tf.ones([self._data_size()]), -1)], axis=1) - \\\n",
    "                        tf.concat([tf.expand_dims(tf.zeros([self._data_size()]), -1), cumul_probs], axis=1)\n",
    "        levels = value\n",
    "        indices_2d = tf.transpose(tf.stack([tf.range(self._data_size()), levels]))  # n x 2\n",
    "        selected_probs = tf.gather_nd(level_probs, indices_2d)\n",
    "        \n",
    "        return tf.reduce_sum(tf.log(selected_probs))\n",
    "    \n",
    "    def _sample_n(self, n, seed=None):\n",
    "        cumul_probs = self._cumul_probs()\n",
    "        new_shape = [n, self._data_size()]\n",
    "        sample_probs = tf.expand_dims(tf.random_uniform(new_shape), -1)\n",
    "        booleans = tf.greater(sample_probs, cumul_probs)  # using broadcasting\n",
    "        samples = tf.reduce_sum(tf.cast(booleans, dtype=dtypes.int32), axis=-1)\n",
    "        return samples\n",
    "    \n",
    "    def mean(self):\n",
    "        # Using the identity that the mean is \\sum_{i=0}^k P(X > i)\n",
    "        cumul_probs = self._cumul_probs()\n",
    "        means = tf.reduce_sum(1-cumul_probs, axis=1)\n",
    "        return means\n",
    "        \n",
    "\n",
    "def __init__(self, *args, **kwargs):\n",
    "    RandomVariable.__init__(self, *args, **kwargs)\n",
    "\n",
    "_name = 'OrdinalLogit'\n",
    "_candidate = distributions_OrdinalLogit\n",
    "__init__.__doc__ = _candidate.__init__.__doc__\n",
    "_globals = globals()\n",
    "_params = {'__doc__': _candidate.__doc__,\n",
    "           '__init__': __init__,\n",
    "           'support': 'countable'}\n",
    "_globals[_name] = type(_name, (RandomVariable, _candidate), _params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed.set_seed(42)\n",
    "\n",
    "# DATA\n",
    "X_all, y_all = build_toy_dataset(2*FLAGS.N, FLAGS.D)\n",
    "\n",
    "test_cutpoints = np.linspace(-1, 1, FLAGS.N_CUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_all)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyz/edward/edward/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "/Users/cyz/edward/edward/lib/python3.6/site-packages/edward/util/random_variables.py:52: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  not np.issubdtype(value.dtype, np.float) and \\\n",
      "/Users/cyz/edward/edward/lib/python3.6/site-packages/edward/util/random_variables.py:53: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  not np.issubdtype(value.dtype, np.int) and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 1s | Loss: 1444.813\n"
     ]
    }
   ],
   "source": [
    "X_train, X_rest, y_train, y_rest = train_test_split(X_all, y_all, train_size=FLAGS.N)\n",
    "\n",
    "# MODEL\n",
    "X = tf.placeholder(tf.float32, [FLAGS.N, FLAGS.D])\n",
    "w = Normal(loc=tf.zeros(FLAGS.D), scale=tf.ones(FLAGS.D))\n",
    "# b = Normal(loc=tf.zeros([1]), scale=tf.ones([1]))\n",
    "# y = Bernoulli(logits=ed.dot(X, w) + b)\n",
    "\n",
    "cutpoints = tf.placeholder(tf.float32, [FLAGS.N_CUT])\n",
    "logits = ed.dot(X, w)\n",
    "y = OrdinalLogit(\n",
    "    logits=logits, \n",
    "    cutpoints=cutpoints, \n",
    "    value=tf.cast(tf.zeros_like(logits), tf.int32)    # avoid sample_n call since we get this error:\n",
    "    # InvalidArgumentError (see above for traceback):  : Tensor had NaN values\n",
    "    # [[Node: inference_1/VerifyFinite_1/CheckNumerics = CheckNumerics[T=DT_FLOAT, _class=[\"loc:@Normal/sample/Reshape\"], message=\"\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](PointMass_1/sample/Reshape)]]\n",
    ")   # b gets absorbed into cutpoints\n",
    "\n",
    "# INFERENCE\n",
    "# qb = Normal(\n",
    "#     loc=tf.Variable(tf.zeros([1])), \n",
    "#     scale=tf.Variable(tf.ones([1])))  # should probably initialize to random values\n",
    "\n",
    "qw = MultivariateNormalTriL(\n",
    "    loc=tf.Variable(tf.random_normal([FLAGS.D])),\n",
    "    scale_tril=tf.Variable(tf.random_normal([FLAGS.D, FLAGS.D])))\n",
    "\n",
    "inference = ed.Laplace({w: qw}, data={X: X_train, y: y_train, cutpoints: test_cutpoints})\n",
    "inference.initialize(n_print=10, n_iter=600)\n",
    "\n",
    "inference.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hmm, is this due to bug or lack of prior on cutpoints?\n",
    "# Replicate Bernoulli with OrdinalLogit...\n",
    "# Wait, no error anymore!\n",
    "# Should still replicate result with Bernoulli as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.6980482e-02,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00],\n",
       "       [ 1.4069458e-03,  1.7247109e-02,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00],\n",
       "       [ 2.3951593e-03,  3.3402701e-03,  1.8310523e-02,  0.0000000e+00,\n",
       "         0.0000000e+00],\n",
       "       [ 2.3831542e-04, -8.8704802e-04, -2.5479004e-03,  1.7532619e-02,\n",
       "         0.0000000e+00],\n",
       "       [-1.2956447e-05,  1.3881541e-04, -3.5206816e-04, -3.0495410e-04,\n",
       "         1.6999261e-02]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these give same result\n",
    "qw.scale.to_dense().eval()\n",
    "tf.cholesky(qw.covariance()).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.8833677e-04,  2.3890618e-05,  4.0670962e-05,  4.0467107e-06,\n",
       "        -2.2000671e-07],\n",
       "       [ 2.3890618e-05,  2.9944227e-04,  6.0979863e-05, -1.4963717e-05,\n",
       "         2.3759353e-06],\n",
       "       [ 4.0670962e-05,  6.0979863e-05,  3.5216945e-04, -4.9045568e-05,\n",
       "        -6.0139041e-06],\n",
       "       [ 4.0467107e-06, -1.4963717e-05, -4.9045568e-05,  3.1472815e-04,\n",
       "        -4.5758329e-06],\n",
       "       [-2.2000671e-07,  2.3759353e-06, -6.0139041e-06, -4.5758329e-06,\n",
       "         2.8921128e-04]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is inverse of observed Fisher information, used in Laplace approximation\n",
    "qw.covariance().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12267312,  0.17115007,  0.3390169 , -0.12927967,  0.00511184],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qw.mean().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qb.scale.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qb.loc.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_fisher = np.linalg.inv(qw.covariance().eval()) / FLAGS.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.5420072  -0.20911463 -0.38912386 -0.11620396 -0.00551768]\n",
      " [-0.20911463  3.4760346  -0.5674161   0.07896247 -0.03926509]\n",
      " [-0.38912386 -0.5674161   3.0472353   0.4539797   0.07491288]\n",
      " [-0.11620396  0.07896247  0.4539797   3.2542145   0.06019045]\n",
      " [-0.00551768 -0.03926509  0.07491288  0.06019045  3.4605083 ]]\n"
     ]
    }
   ],
   "source": [
    "print(obs_fisher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.log_prob requires argument of length y\n",
    "# construct a new variable representing candidate from y_rest\n",
    "# compute likelihood with autodiff\n",
    "\n",
    "X = tf.placeholder(tf.float32, [1, FLAGS.D])\n",
    "w = Normal(loc=tf.zeros(FLAGS.D), scale=tf.ones(FLAGS.D))\n",
    "y = OrdinalLogit(logits=ed.dot(X, w), cutpoints=cutpoints)   # b gets absorbed into cutpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_next = tf.get_variable(\"y_next\", [1], dtype=dtypes.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_next_hess = tf.hessians(y.log_prob(y_next.value()), w)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_map = qw.mean().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed Fisher information, see https://en.wikipedia.org/wiki/Observed_information\n",
    "def new_point_info(X_new, y_new):\n",
    "    return -y_next_hess.eval(feed_dict={X: X_new, w: w_map, cutpoints: test_cutpoints, y_next: y_new})\n",
    "\n",
    "# actual Fisher information (expectation taken over y_new)\n",
    "# TODO: make this more efficient (e.g. don't run all calls to new_point_info in separate sessions)\n",
    "def new_point_info_expected(X_new):\n",
    "    # integrate out y_next\n",
    "    cumul_probs = y._cumul_probs().eval(feed_dict={X: X_new, w: w_map, cutpoints: test_cutpoints})[0]\n",
    "    probs = np.append(cumul_probs, 1) - np.append(0, cumul_probs)\n",
    "    levels = np.arange(probs.size)\n",
    "    info_per_level = np.array([new_point_info(X_new, [level]) for level in levels])\n",
    "    return probs[:,np.newaxis,np.newaxis] * info_per_level\n",
    "\n",
    "## TODO: should initialize to the prior precision\n",
    "obs_fisher_2 = np.zeros((FLAGS.D, FLAGS.D))\n",
    "for i in range(len(X_train)):\n",
    "    obs_fisher_2 += new_point_info(X_train[[i]], y_train[[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.46716672, -0.28291825, -0.40205225, -0.11988337, -0.0055238 ],\n",
       "       [-0.28291825,  3.38384442, -0.58046724,  0.09074351, -0.03825025],\n",
       "       [-0.40205228, -0.58046724,  3.14010583,  0.43257301,  0.07724843],\n",
       "       [-0.11988333,  0.09074349,  0.43257302,  3.32125017,  0.06772527],\n",
       "       [-0.00552376, -0.03825023,  0.0772485 ,  0.06772528,  3.462631  ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_fisher_2 / FLAGS.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's implement item selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "OptCriteria = namedtuple('OptCriteria', ['trace', 'logdet', 'max_eigval'])\n",
    "\n",
    "# Lambda is the precision matrix\n",
    "# return measures of the variance, Lambda^{-1}\n",
    "def compute_opt_criteria(Lambda):\n",
    "    eigvals_Lambda = np.linalg.eigvals(Lambda)\n",
    "    # When there are not enough previous questions for full rank,\n",
    "    # some eigenvalues will be 0. Ensure numerical stability here.\n",
    "    # Originally chose threshold 1e-8, but sometimes an eigenvalue\n",
    "    # that should've been 0 exceeded this threshold.\n",
    "    # TODO: handle in a safer way.\n",
    "    eigvals_Lambda[eigvals_Lambda < 1e-6] = 1e-6\n",
    "    eigvals_Var = 1.0 / eigvals_Lambda\n",
    "    return OptCriteria(\n",
    "        trace = np.sum(eigvals_Var),\n",
    "        logdet = np.sum(np.log(eigvals_Var)),\n",
    "        max_eigval = np.max(eigvals_Var)\n",
    "    )\n",
    "\n",
    "opt_criteria_comparators = dict(\n",
    "    A = lambda o1, o2: o1.trace - o2.trace,\n",
    "    D = lambda o1, o2: o1.logdet - o2.logdet,\n",
    "    E = lambda o1, o2: o1.max_eigval - o2.max_eigval\n",
    ")\n",
    "\n",
    "def cmp_criteria_for(optimality_type):\n",
    "    return opt_criteria_comparators[optimality_type]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Lambda_prev = obs_fisher_2\n",
    "best_new, best_opt_criteria = None, None\n",
    "cmp_criteria = cmp_criteria_for('A')\n",
    "\n",
    "for i in range(min(100, FLAGS.N)):\n",
    "    X_cand = X_rest[[i]]\n",
    "    Lambda_cand = Lambda_prev + new_point_info_expected(X_cand)\n",
    "    opt_criteria = compute_opt_criteria(Lambda_cand)\n",
    "    \n",
    "    if best_opt_criteria is None or cmp_criteria(opt_criteria, best_opt_criteria) < 0:\n",
    "        best_new = i\n",
    "        best_opt_criteria = opt_criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57,\n",
       " array([[ 3.72820284,  5.38043463, -4.90409119, -4.06262777,  5.52895563]]),\n",
       " array([0]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_new, X_rest[[best_new]], y_rest[[best_new]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edward",
   "language": "python",
   "name": "edward"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
