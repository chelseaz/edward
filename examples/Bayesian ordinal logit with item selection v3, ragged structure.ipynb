{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pprint\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from edward.models import Bernoulli, Dirichlet, Normal, MultivariateNormalTriL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLAGS:\n",
    "    M=60   # Number of questions\n",
    "    D=3     # Number of features\n",
    "    # length-M vector representing number of cutpoints per question\n",
    "#     N_CUT_VEC = np.repeat(5, M)\n",
    "    N_CUT_VEC = np.repeat(np.arange(3, 7)[:,np.newaxis], 15, axis=1).T.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_toy_dataset(N, D, noise_std=1, max_level_vec=FLAGS.N_CUT_VEC):\n",
    "    X = np.random.uniform(-6, 6, size=(N, D))\n",
    "    w = np.random.uniform(-1, 1, size=D)\n",
    "    epsilon = np.random.normal(0, noise_std, size=N)\n",
    "    \n",
    "    # levels will be 0, 1, ..., max_level per question\n",
    "    max_level = max_level_vec.max()\n",
    "    levels = np.array(range(max_level))\n",
    "\n",
    "    # note this is not generated from ordered logit model\n",
    "    thresholds = levels[:,np.newaxis]\n",
    "    logits = np.dot(X, w) + epsilon\n",
    "    y = (logits > thresholds).astype(int).sum(axis=0)\n",
    "    # level is the last TRUE in each row, which is also the row sum\n",
    "    # now clip to maximum level per question\n",
    "    y = np.round(np.clip(y, 0, max_level_vec)).astype(int)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def get_indicators(N, prob_std=0.5):\n",
    "    ind = np.random.binomial(1, prob_std, N)\n",
    "    return ind\n",
    "\n",
    "def logit(p):\n",
    "    return tf.log(p / (1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edward.models import RandomVariable\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.contrib.distributions import Distribution, NOT_REPARAMETERIZED\n",
    "\n",
    "# Inputs are matrices, but value of random variable is a vector \n",
    "# whose length is the number of revealed entries (data_size).\n",
    "# This is to accommodate a different number of cutpoints per question.\n",
    "class distributions_OrdinalLogit(Distribution):\n",
    "    def __init__(self, logits, cutpoints, indicators, name=\"OrdinalLogit\"):\n",
    "        self._logits = logits  # N x K tensor\n",
    "        self._cutpoints = cutpoints  # K x max(N_CUT_VEC) tensor\n",
    "        self._indicators = indicators  # N x K tensor\n",
    "        \n",
    "        parameters = dict(locals())\n",
    "        super(distributions_OrdinalLogit, self).__init__(\n",
    "            dtype=dtypes.int32,\n",
    "            reparameterization_type=NOT_REPARAMETERIZED,\n",
    "            validate_args=False,\n",
    "            allow_nan_stats=True,\n",
    "            parameters=parameters,\n",
    "            graph_parents=[self._logits, self._cutpoints, self._indicators],\n",
    "            name=name\n",
    "        )\n",
    "\n",
    "    def _cumul_probs(self):\n",
    "        # form a N x K x max(N_CUT_VEC) tensor representing sum of logits and cutpoints\n",
    "        # use broadcasting\n",
    "        logits_with_cutpoints = tf.expand_dims(self._logits, -1) + tf.expand_dims(self._cutpoints, 0)\n",
    "        # now subset to a data_size x max(N_CUT_VEC) array using indicators\n",
    "        select_logits_with_cutpoints = tf.boolean_mask(logits_with_cutpoints, self._indicators)\n",
    "        return tf.sigmoid(select_logits_with_cutpoints)  # data_size x max(N_CUT_VEC)\n",
    "        \n",
    "    def _log_prob(self, value):        \n",
    "        cumul_probs = self._cumul_probs()\n",
    "        data_size = tf.shape(cumul_probs)[0]\n",
    "        level_probs = tf.concat([cumul_probs, tf.expand_dims(tf.ones([data_size]), -1)], axis=1) - \\\n",
    "                        tf.concat([tf.expand_dims(tf.zeros([data_size]), -1), cumul_probs], axis=1)\n",
    "        levels = value\n",
    "        indices_2d = tf.transpose(tf.stack([tf.range(data_size), levels]))  # n x 2\n",
    "        selected_probs = tf.gather_nd(level_probs, indices_2d)\n",
    "        \n",
    "        return tf.reduce_sum(tf.log(selected_probs))\n",
    "    \n",
    "    def _sample_n(self, n, seed=None):\n",
    "        cumul_probs = self._cumul_probs()\n",
    "        data_size = tf.shape(cumul_probs)[0]\n",
    "        new_shape = [n, data_size]\n",
    "        sample_probs = tf.expand_dims(tf.random_uniform(new_shape), -1)\n",
    "        booleans = tf.greater(sample_probs, cumul_probs)  # using broadcasting\n",
    "        samples = tf.reduce_sum(tf.cast(booleans, dtype=dtypes.int32), axis=-1)\n",
    "        return samples\n",
    "    \n",
    "    def mean(self):\n",
    "        # Using the identity that the mean is \\sum_{i=0}^k P(X > i)\n",
    "        cumul_probs = self._cumul_probs()\n",
    "        means = tf.reduce_sum(1-cumul_probs, axis=1)\n",
    "        return means\n",
    "        \n",
    "\n",
    "def __init__(self, *args, **kwargs):\n",
    "    RandomVariable.__init__(self, *args, **kwargs)\n",
    "\n",
    "_name = 'OrdinalLogit'\n",
    "_candidate = distributions_OrdinalLogit\n",
    "__init__.__doc__ = _candidate.__init__.__doc__\n",
    "_globals = globals()\n",
    "_params = {'__doc__': _candidate.__doc__,\n",
    "           '__init__': __init__,\n",
    "           'support': 'countable'}\n",
    "_globals[_name] = type(_name, (RandomVariable, _candidate), _params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ed.set_seed(2019)\n",
    "\n",
    "# DATA\n",
    "X_all, y_all = build_toy_dataset(FLAGS.M, FLAGS.D)\n",
    "I_train = get_indicators(FLAGS.M).astype(bool)\n",
    "y_train = y_all[I_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_all)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyz/edward/edward/lib/python3.6/site-packages/edward/util/random_variables.py:52: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  not np.issubdtype(value.dtype, np.float) and \\\n",
      "/Users/cyz/edward/edward/lib/python3.6/site-packages/edward/util/random_variables.py:53: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  not np.issubdtype(value.dtype, np.int) and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 1s | Loss: 23.832\n"
     ]
    }
   ],
   "source": [
    "# MODEL\n",
    "X = tf.placeholder(tf.float32, [FLAGS.M, FLAGS.D])\n",
    "w = Normal(loc=tf.zeros(FLAGS.D), scale=tf.ones(FLAGS.D))\n",
    "I = tf.placeholder(tf.float32, [1, FLAGS.M])\n",
    "\n",
    "logits = tf.expand_dims(ed.dot(X, w), 0)\n",
    "\n",
    "# Using Dirichlet prior to define cutpoints, per rstanarm:\n",
    "# https://cran.r-project.org/web/packages/rstanarm/vignettes/polr.html\n",
    "all_ones = tf.ones(shape=(FLAGS.N_CUT_VEC.size, FLAGS.N_CUT_VEC.max()+1), dtype=dtypes.int64)\n",
    "all_zeros = tf.zeros(shape=all_ones.shape, dtype=dtypes.int64)\n",
    "range_per_row = tf.cumsum(all_ones, axis=1) - 1\n",
    "keep_one_mask = tf.less_equal(range_per_row, tf.expand_dims(FLAGS.N_CUT_VEC, 1))\n",
    "# inflate concentration parameters to get closer to uniform\n",
    "# if we don't reduce noise this way, ELBO optimization may not converge\n",
    "conc_multiple = 2\n",
    "concentration = tf.cast(tf.where(keep_one_mask, all_ones * conc_multiple, all_zeros), dtypes.float32)\n",
    "pi = Dirichlet(concentration)\n",
    "cumpi = tf.cumsum(pi, axis=1)\n",
    "cutpoints = logit(tf.minimum(cumpi, 1-1e-6))[:,:-1]\n",
    "\n",
    "y = OrdinalLogit(\n",
    "    logits=logits, \n",
    "    cutpoints=cutpoints,\n",
    "    indicators=I)\n",
    "\n",
    "# INFERENCE\n",
    "# Using MAP inference as Laplace inference results in NaN errors,\n",
    "# probably due to random initialization of qw covariance matrix\n",
    "qw = ed.models.PointMass(params=tf.Variable(tf.zeros(FLAGS.D)))\n",
    "inference = ed.MAP({w: qw}, data={X: X_all, y: y_train, I: I_train[np.newaxis,:]})\n",
    "inference.initialize(n_print=10, n_iter=600)\n",
    "inference.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.7688644, -0.9285711, -0.4510173], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qw.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.log_prob requires argument of length y\n",
    "# construct a new variable representing candidate from y_rest\n",
    "# compute likelihood with autodiff\n",
    "\n",
    "X = tf.placeholder(tf.float32, [1, FLAGS.D])\n",
    "w = tf.placeholder(tf.float32, [FLAGS.D])\n",
    "single_q_cutpoints = tf.placeholder(tf.float32, [1, cutpoints.shape[1].value])\n",
    "y = OrdinalLogit(\n",
    "    logits=tf.matmul(X, tf.expand_dims(w, -1)), \n",
    "    cutpoints=single_q_cutpoints,\n",
    "    indicators=tf.ones((1, 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_next = tf.get_variable(\"y_next\", [1], dtype=dtypes.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_next_hess = tf.hessians(y.log_prob(y_next.value()), w)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_map = qw.mean().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observed Fisher information, see https://en.wikipedia.org/wiki/Observed_information\n",
    "def new_point_info(X_new, y_new, cutpoints):\n",
    "    return -y_next_hess.eval(feed_dict={X: X_new, w: w_map, single_q_cutpoints: cutpoints, y_next: y_new})\n",
    "\n",
    "# actual Fisher information (expectation taken over y_new)\n",
    "# TODO: make this more efficient (e.g. don't run all calls to new_point_info in separate sessions)\n",
    "def new_point_info_expected(X_new, max_level, cutpoints):\n",
    "    # integrate out y_next\n",
    "    cumul_probs = y._cumul_probs().eval(feed_dict={X: X_new, w: w_map, single_q_cutpoints: cutpoints})[0]\n",
    "    probs = np.append(cumul_probs, 1) - np.append(0, cumul_probs)\n",
    "    levels = np.arange(max_level+1)\n",
    "    info_per_level = np.array([new_point_info(X_new, [level], cutpoints) for level in levels])\n",
    "    return np.sum(probs[:(max_level+1),np.newaxis,np.newaxis] * info_per_level, axis=0)\n",
    "\n",
    "# # Surprise! This implementation is actually slower\n",
    "# def new_point_info_expected(X_new, max_level, cutpoints):\n",
    "#     # integrate out y_next\n",
    "#     cumul_probs = y._cumul_probs()[0]\n",
    "#     probs = tf.concat([cumul_probs, [1]], axis=0) - tf.concat([[0], cumul_probs], axis=0)\n",
    "    \n",
    "#     y_next_all = [tf.Variable(tf.zeros(1, dtype=dtypes.int32)) for i in range(max_level+1)]\n",
    "#     y_next_hess_all = [tf.hessians(y.log_prob(y_next_all[i].value()), w)[0]\n",
    "#                    for i in range(max_level+1)]\n",
    "#     new_point_info_all = -tf.stack(y_next_hess_all)\n",
    "#     expected_info = tf.reduce_sum(\n",
    "#         tf.expand_dims(tf.expand_dims(probs[:(max_level+1)],1),1) * new_point_info_all,\n",
    "#         axis=0)\n",
    "    \n",
    "#     feed_dict = {X: X_new, w: w_map, single_q_cutpoints: cutpoints}\n",
    "#     feed_dict.update({y_next: [level] for level, y_next in enumerate(y_next_all)})\n",
    "#     return expected_info.eval(feed_dict=feed_dict)\n",
    "\n",
    "## TODO: should initialize to the prior precision\n",
    "obs_fisher = np.zeros((FLAGS.D, FLAGS.D))\n",
    "obs_idx = np.where(I_train)[0]\n",
    "for i in obs_idx:\n",
    "    cutpoints_i = cutpoints[i].eval()[np.newaxis,:]\n",
    "    obs_fisher += new_point_info(X_all[[i]], y_all[[i]], cutpoints_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.89906835, -0.40658859,  0.14962822],\n",
       "       [-0.4065885 ,  1.15278743, -0.48966041],\n",
       "       [ 0.14962824, -0.48966049,  1.13272203]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_fisher / I_train.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's implement item selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "OptCriteria = namedtuple('OptCriteria', ['trace', 'logdet', 'max_eigval'])\n",
    "\n",
    "# Lambda is the precision matrix\n",
    "# return measures of the variance, Lambda^{-1}\n",
    "def compute_opt_criteria(Lambda):\n",
    "    eigvals_Lambda = np.linalg.eigvals(Lambda)\n",
    "    # When there are not enough previous questions for full rank,\n",
    "    # some eigenvalues will be 0. Ensure numerical stability here.\n",
    "    # Originally chose threshold 1e-8, but sometimes an eigenvalue\n",
    "    # that should've been 0 exceeded this threshold.\n",
    "    # TODO: handle in a safer way.\n",
    "    eigvals_Lambda[eigvals_Lambda < 1e-6] = 1e-6\n",
    "    eigvals_Var = 1.0 / eigvals_Lambda\n",
    "    return OptCriteria(\n",
    "        trace = np.sum(eigvals_Var),\n",
    "        logdet = np.sum(np.log(eigvals_Var)),\n",
    "        max_eigval = np.max(eigvals_Var)\n",
    "    )\n",
    "\n",
    "opt_criteria_comparators = dict(\n",
    "    A = lambda o1, o2: o1.trace - o2.trace,\n",
    "    D = lambda o1, o2: o1.logdet - o2.logdet,\n",
    "    E = lambda o1, o2: o1.max_eigval - o2.max_eigval\n",
    ")\n",
    "\n",
    "def cmp_criteria_for(optimality_type):\n",
    "    return opt_criteria_comparators[optimality_type]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Lambda_prev = obs_fisher\n",
    "best_new, best_opt_criteria = None, None\n",
    "cmp_criteria = cmp_criteria_for('A')\n",
    "\n",
    "unobs_idx = np.where(~I_train)[0]\n",
    "for i in unobs_idx:\n",
    "    X_cand = X_all[[i]]\n",
    "    max_level = FLAGS.N_CUT_VEC[i]\n",
    "    cutpoints_i = cutpoints[i].eval()[np.newaxis,:]\n",
    "    Lambda_cand = Lambda_prev + new_point_info_expected(X_cand, max_level, cutpoints_i)\n",
    "    opt_criteria = compute_opt_criteria(Lambda_cand)\n",
    "    \n",
    "    if best_opt_criteria is None or cmp_criteria(opt_criteria, best_opt_criteria) < 0:\n",
    "        best_new = i\n",
    "        best_opt_criteria = opt_criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, array([[-5.84776736,  3.61137122,  2.59459346]]), array([0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_new, X_all[[best_new]], y_all[[best_new]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edward",
   "language": "python",
   "name": "edward"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
