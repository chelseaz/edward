{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pprint\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from edward.models import Bernoulli, Normal, MultivariateNormalTriL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLAGS:\n",
    "    N=1000   # Number of data points\n",
    "    D=5     # Number of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_toy_dataset(N, D, noise_std=1):\n",
    "    X = np.random.uniform(-6, 6, size=(N, D))\n",
    "    w = np.random.uniform(-1, 1, size=D)\n",
    "    b = np.random.uniform(-4, 4)\n",
    "    epsilon = np.random.normal(0, noise_std, size=N)\n",
    "    y = (np.dot(X, w) + b + epsilon > 0).astype(int)\n",
    "    # note this is actually generated from a probit model\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyz/edward/edward/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.45701382  0.22541014  1.6967911  -1.13522536 -1.30567135]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyz/edward/edward/lib/python3.6/site-packages/edward/util/random_variables.py:52: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  not np.issubdtype(value.dtype, np.float) and \\\n",
      "/Users/cyz/edward/edward/lib/python3.6/site-packages/edward/util/random_variables.py:53: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  not np.issubdtype(value.dtype, np.int) and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 1s | Loss: 161.571\n"
     ]
    }
   ],
   "source": [
    "ed.set_seed(42)\n",
    "\n",
    "# DATA\n",
    "X_all, y_all = build_toy_dataset(2*FLAGS.N, FLAGS.D)\n",
    "X_train, X_next, y_train, y_next = train_test_split(X_all, y_all, train_size=FLAGS.N)\n",
    "\n",
    "# MODEL\n",
    "X = tf.placeholder(tf.float32, [FLAGS.N, FLAGS.D])\n",
    "w = Normal(loc=tf.zeros(FLAGS.D), scale=tf.ones(FLAGS.D))\n",
    "b = Normal(loc=tf.zeros([1]), scale=tf.ones([1]))\n",
    "y = Bernoulli(logits=ed.dot(X, w) + b)\n",
    "\n",
    "# INFERENCE\n",
    "qb = Normal(\n",
    "    loc=tf.Variable(tf.zeros([1])), \n",
    "    scale=tf.Variable(tf.ones([1])))  # should probably initialize to random values\n",
    "\n",
    "w_init = np.random.randn(FLAGS.D)\n",
    "print(w_init)\n",
    "\n",
    "qw = MultivariateNormalTriL(\n",
    "    loc=tf.Variable(tf.cast(w_init, tf.float32)),\n",
    "    scale_tril=tf.Variable(tf.random_normal([FLAGS.D, FLAGS.D])))\n",
    "\n",
    "# inference = ed.KLqp({w: qw, b: qb}, data={X: X_train, y: y_train})\n",
    "inference = ed.Laplace({w: qw, b: qb}, data={X: X_train, y: y_train})\n",
    "inference.initialize(n_print=10, n_iter=600)\n",
    "\n",
    "inference.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAGS.D == 1:\n",
    "    n_posterior_samples = 10\n",
    "\n",
    "    w_post = qw.sample(n_posterior_samples).eval()\n",
    "    b_post = qb.sample(n_posterior_samples).eval()\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (8,6)\n",
    "    plt.scatter(X_train, y_train)\n",
    "\n",
    "    inputs = np.linspace(-6, 6, num=400)\n",
    "    for ns in range(n_posterior_samples):\n",
    "        output = scipy.special.expit(np.dot(inputs[:,np.newaxis], w_post[ns]) + b_post[ns])\n",
    "        plt.plot(inputs, output)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04134055,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.00395684,  0.04146119,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.00021316,  0.00259639,  0.03781373,  0.        ,  0.        ],\n",
       "       [-0.00136431, -0.00312252,  0.00105848,  0.04131312,  0.        ],\n",
       "       [ 0.00027188,  0.00444304, -0.00186874, -0.00429154,  0.04052261]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these give same result\n",
    "qw.scale.to_dense().eval()\n",
    "tf.cholesky(qw.covariance()).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.7090413e-03,  1.6357777e-04, -8.8121587e-06, -5.6401255e-05,\n",
       "         1.1239480e-05],\n",
       "       [ 1.6357777e-04,  1.7346867e-03,  1.0680584e-04, -1.3486191e-04,\n",
       "         1.8528968e-04],\n",
       "       [-8.8121587e-06,  1.0680584e-04,  1.4366646e-03,  3.2208438e-05,\n",
       "        -5.9186139e-05],\n",
       "       [-5.6401255e-05, -1.3486191e-04,  3.2208438e-05,  1.7195056e-03,\n",
       "        -1.9351946e-04],\n",
       "       [ 1.1239480e-05,  1.8528968e-04, -5.9186139e-05, -1.9351946e-04,\n",
       "         1.6838056e-03]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is inverse of observed Fisher information, used in Laplace approximation\n",
    "qw.covariance().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.415916  , -0.49535665, -0.9982332 ,  0.38619578, -0.05535197],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qw.mean().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01915632], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qb.scale.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.6540523], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qb.loc.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify Segall formula (with c_i=0) is the same\n",
    "w_map = qw.mean().eval()\n",
    "b_map = qb.loc.eval()\n",
    "p = y.mean().eval(feed_dict={X: X_train, w: w_map, b: b_map})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.diag(p * (1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this is the inverse covariance for weighted least squares! The weights are just Bernoulli variances\n",
    "hess_segall = np.matmul(np.matmul(X_train.T, W), X_train) / FLAGS.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.58412335 -0.05584107  0.00713261  0.01491958  0.00410587]\n",
      " [-0.05584107  0.58605255 -0.04062332  0.04356729 -0.06125131]\n",
      " [ 0.00713261 -0.04062332  0.70118987 -0.02082551  0.03458291]\n",
      " [ 0.01491958  0.04356729 -0.02082551  0.58921989  0.05666981]\n",
      " [ 0.00410587 -0.06125131  0.03458291  0.05666981  0.62226782]]\n"
     ]
    }
   ],
   "source": [
    "print(hess_segall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_fisher = np.linalg.inv(qw.covariance().eval()) / FLAGS.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5909487  -0.05544468  0.00757476  0.0153646   0.00418875]\n",
      " [-0.05544468  0.59431267 -0.04795557  0.03868396 -0.06226909]\n",
      " [ 0.00757476 -0.04795557  0.7011369  -0.01345808  0.02832496]\n",
      " [ 0.0153646   0.03868396 -0.01345808  0.59247214  0.06326022]\n",
      " [ 0.00418875 -0.06226909  0.02832496  0.06326022  0.60898316]]\n"
     ]
    }
   ],
   "source": [
    "print(obs_fisher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The expression Segall derived for the Fisher information is close enough \n",
    "# to the observed Fisher information computed using TF autodiff, available from Laplace approximation.\n",
    "# What causes the discrepancy?\n",
    "# Is the Edward Laplace approx really computed at the mode? We evaluate the Segall expression at the MAP estimate...\n",
    "\n",
    "# Discrepancy increases if the number of samples is one order of magnitude more/less (not sure why)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.log_prob requires argument of length y\n",
    "# construct a new variable representing candidate from y_next\n",
    "# compute likelihood with autodiff\n",
    "\n",
    "X = tf.placeholder(tf.float32, [1, FLAGS.D])\n",
    "w = Normal(loc=tf.zeros(FLAGS.D), scale=tf.ones(FLAGS.D))\n",
    "b = Normal(loc=tf.zeros([1]), scale=tf.ones([1]))\n",
    "y = Bernoulli(logits=ed.dot(X, w) + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_next = tf.get_variable(\"y_next\", [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_next_hess = tf.hessians(y.log_prob(y_next.value()), w)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# y_next_hess.eval(feed_dict={X: X_next[[0]], w: w_map, b: b_map})\n",
    "# hessian doesn't involve y_next, so don't need to feed it in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_point_info(X_new):\n",
    "    return -y_next_hess.eval(feed_dict={X: X_new, w: w_map, b: b_map})\n",
    "\n",
    "obs_fisher_2 = np.zeros((FLAGS.D, FLAGS.D))\n",
    "for i in range(len(X_train)):\n",
    "    obs_fisher_2 += new_point_info(X_train[[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.58412336, -0.05584107,  0.00713259,  0.01491957,  0.00410587],\n",
       "       [-0.05584107,  0.58605253, -0.0406233 ,  0.04356728, -0.06125131],\n",
       "       [ 0.0071326 , -0.0406233 ,  0.70118985, -0.02082552,  0.03458291],\n",
       "       [ 0.01491957,  0.04356728, -0.02082552,  0.58921987,  0.05666981],\n",
       "       [ 0.00410587, -0.06125132,  0.03458291,  0.05666981,  0.62226782]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_fisher_2 / FLAGS.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.58412335, -0.05584107,  0.00713261,  0.01491958,  0.00410587],\n",
       "       [-0.05584107,  0.58605255, -0.04062332,  0.04356729, -0.06125131],\n",
       "       [ 0.00713261, -0.04062332,  0.70118987, -0.02082551,  0.03458291],\n",
       "       [ 0.01491958,  0.04356729, -0.02082551,  0.58921989,  0.05666981],\n",
       "       [ 0.00410587, -0.06125131,  0.03458291,  0.05666981,  0.62226782]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hess_segall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yay, we match Segall exactly if we compute the Hessians with autodiff ourselves, \n",
    "# instead of using the observed Fisher information from the Laplace approximation!\n",
    "# Now let's implement item selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "OptCriteria = namedtuple('OptCriteria', ['trace', 'logdet', 'max_eigval'])\n",
    "\n",
    "# Lambda is the precision matrix\n",
    "# return measures of the variance, Lambda^{-1}\n",
    "def compute_opt_criteria(Lambda):\n",
    "    eigvals_Lambda = np.linalg.eigvals(Lambda)\n",
    "    # When there are not enough previous questions for full rank,\n",
    "    # some eigenvalues will be 0. Ensure numerical stability here.\n",
    "    # Originally chose threshold 1e-8, but sometimes an eigenvalue\n",
    "    # that should've been 0 exceeded this threshold.\n",
    "    # TODO: handle in a safer way.\n",
    "    eigvals_Lambda[eigvals_Lambda < 1e-6] = 1e-6\n",
    "    eigvals_Var = 1.0 / eigvals_Lambda\n",
    "    return OptCriteria(\n",
    "        trace = np.sum(eigvals_Var),\n",
    "        logdet = np.sum(np.log(eigvals_Var)),\n",
    "        max_eigval = np.max(eigvals_Var)\n",
    "    )\n",
    "\n",
    "opt_criteria_comparators = dict(\n",
    "    A = lambda o1, o2: o1.trace - o2.trace,\n",
    "    D = lambda o1, o2: o1.logdet - o2.logdet,\n",
    "    E = lambda o1, o2: o1.max_eigval - o2.max_eigval\n",
    ")\n",
    "\n",
    "def cmp_criteria_for(optimality_type):\n",
    "    return opt_criteria_comparators[optimality_type]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda_prev = obs_fisher_2\n",
    "best_new, best_opt_criteria = None, None\n",
    "cmp_criteria = cmp_criteria_for('A')\n",
    "\n",
    "for i in range(FLAGS.N):\n",
    "    X_cand = X_next[[i]]\n",
    "    Lambda_cand = Lambda_prev + new_point_info(X_cand)\n",
    "    opt_criteria = compute_opt_criteria(Lambda_cand)\n",
    "    \n",
    "    if best_opt_criteria is None or cmp_criteria(opt_criteria, best_opt_criteria) < 0:\n",
    "        best_new = i\n",
    "        best_opt_criteria = opt_criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(259,\n",
       " array([[ 4.09315405,  5.96219141, -0.83588192, -2.15641509,  4.87983956]]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_new, X_next[[best_new]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edward",
   "language": "python",
   "name": "edward"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
